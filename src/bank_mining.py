# -*- coding: utf-8 -*-
"""bank_mining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PbrPE9kdgjmWwFoL5vYV4ngjky3izJPS

# Data Mining Project - Bank

## Project Developed by:
- *Duarte Sardão*
- *Gabriel Ferreira*
- *Miguel Lopes*
- *Sérgio Estêvão*

## Table of Contents
1. Business Understanding

## Business Understanding

The dataset is composed of a series of information that describe the activity of a Czech bank during the 90s. This includes account, client, credit cards, transaction and loan information as well as information regarding the districts where the bank's clients reside.

With this information we are expected to create a predictive model that allows the end user to determine whether or not a client is suitable to get a loan. 

In this business context, a client is granted a loan if the model predicts that the client will be able to pay it in full.

Taking into account that an unpaid loan can result in up to 100% loss, while interest is unlikely to account for 100% profit over the loan value, an unpaid loan incurrs much higher negative value than a loan that was not granted. Thus in this case, our goal should be to minimize unpaid loans and thus, as relates to data analysis: minimize the number of false positives.
"""

import pandas as pd
import datetime as dt
import matplotlib.pyplot as plt
import numpy as np

"""We start by loading the data into dataframes and removing information that has no relation to loans, as that data is irrelevant to the problem being targeted in this project."""

# loan_id;account_id;date;amount;duration;payments;status
loans = pd.read_csv("./data/loan_dev.csv", sep=";",dtype=int)
transactions = pd.read_csv("./data/trans_dev.csv", sep=";",dtype={"trans_id":int,"account_id":int,"date":int,"type":str,"operation":str,"amount":float,"balance":float,"k_symbol":str,"bank":str,"account":str})
accounts = pd.read_csv("./data/account.csv", sep=";",dtype={"account_id":int,"district_id":int,"frequency":str,"date":int})
cards = pd.read_csv("./data/card_dev.csv", sep=";",dtype={"card_id":int,"disp_id":int,"type":str,"issued":int})
clients = pd.read_csv("./data/client.csv", sep=";",dtype=int)
dispositions = pd.read_csv("./data/disp.csv", sep=";",dtype={"disp_id":int,"client_id":int,"account_id":int,"type":str})
districts = pd.read_csv("./data/district.csv", na_values=['?'], sep=";",dtype={"code":int, "name":str, "region":str,"no. of inhabitants":int,"no. of municipalities with inhabitants < 499":int,"no. of municipalities with inhabitants 500-1999":int,"no. of municipalities with inhabitants 2000-9999":int, "no. of municipalities with inhabitants >10000": int, "no. of cities":int, "ratio of urban inhabitants":float, "average salary":float, "unemploymant rate '95":float, "unemploymant rate '96":float, "no. of enterpreneurs per 1000 inhabitants": float, "no. of commited crimes '95":int, "no. of commited crimes '96":int})

print("\n==== Before Clean-up ====")
print("Transactions: ", len(transactions))
print("Accounts: ", len(accounts))
print("Dispositions: ", len(dispositions))
print("Clients: ", len(clients))
print("Cards: ", len(cards))
print("Districts: ", len(districts))

#Clean up information not associated with loans

account_ids = loans['account_id'].unique()

transactions.query("account_id in @account_ids", inplace=True)
accounts.query("account_id in @account_ids", inplace=True)
dispositions.query("account_id in @account_ids", inplace=True)

#save how many clients are associated to an account, might matter
accounts['num_clients'] = accounts.apply(lambda row: dispositions['account_id'].value_counts()[row.account_id], axis=1)


#but drop their info
dispositions.query("type == 'OWNER'", inplace=True)

client_ids = dispositions['client_id'].unique()
disp_ids = dispositions['disp_id'].unique()
district_ids = accounts['district_id'].unique()

#rename cuz of ending spaces
districts = districts.rename(columns={"code ": "code",
                   "name ": "name"})

clients.query("client_id in @client_ids", inplace=True)
cards.query("disp_id in @disp_ids", inplace=True)
districts.query("code in @district_ids", inplace=True)

print("\n==== After Clean-up ====")
print("Transactions: ", len(transactions))
print("Accounts: ", len(accounts))
print("Dispositions: ", len(dispositions))
print("Clients: ", len(clients))
print("Cards: ", len(cards))
print("Districts: ", len(districts))

"""Next we start to create a unified data-frame. Starting by merging the Disposition and Accounts data-frames into the Clients dataset."""

dispositions_accounts = pd.merge(dispositions, accounts, on="account_id")
dispositions_accounts.drop(columns="type", inplace=True)
dispositions_accounts = dispositions_accounts.rename(columns={"date": "creation", "district_id": "branch_district_id"})
#current_date = datetime.strptime("1997-01-01", '%Y-%m-%d')
#dispositions_accounts["account_age"] = dispositions_accounts["creation"].map(lambda dt: math.floor((current_date - dt).days/365.2425))
clients = pd.merge(dispositions_accounts, clients, on="client_id")
clients = clients.rename(columns={"district_id": "client_district_id"})
clients.head()

"""After that, and still on the Clients dataset, we parse the birth_number which contains information about the clients birth date and sex. Also we calculate the age of the clients considering "1997-01-01" as the current time."""

from datetime import datetime
import math
#transform birth number into birthdate and sex
birth_number = clients["birth_number"]
clients["sex"] = birth_number.map(lambda nr: "Male" if nr % 10000 < 5000 else "Female").astype('category')
clients["birthdate"] = birth_number.map(lambda nr: str(nr) if nr % 10000 < 5000 else str(nr-5000))
clients["birthdate"] = pd.to_datetime("19"+clients["birthdate"], format='%Y%m%d')

current_date = datetime.strptime("1997-01-01", '%Y-%m-%d')
birthdates = clients["birthdate"]
clients["age"] = birthdates.map(lambda dt: math.floor((current_date - dt).days/365.2425))

clients.drop(columns="birth_number", inplace=True)
clients.head()

"""We then parse other dates, converting them from the provided format into the YYYY-MM-DD format"""

#other date treatment (yymmdd string to datetime format)
cards["issued"] = cards["issued"].map(lambda nr: str(nr))
cards["issued"] = pd.to_datetime("19"+cards["issued"], format='%Y%m%d')

transactions["date"] = transactions["date"].map(lambda nr: str(nr))
transactions["date"] = pd.to_datetime("19"+transactions["date"], format='%Y%m%d')

loans["date"] = loans["date"].map(lambda nr: str(nr))
loans["date"] = pd.to_datetime("19"+loans["date"], format='%Y%m%d')

clients["creation"] = clients["creation"].map(lambda nr: str(nr))
clients["creation"] = pd.to_datetime("19"+clients["creation"], format='%Y%m%d')

"""We then go on to merge the card info into the Clients dataset"""

clients = pd.merge(clients, cards, on="disp_id", how="outer")
clients.drop(columns="card_id", inplace=True)
clients = clients.rename(columns={"type": "card_type","issued": "card_issue"})
clients.head()

"""here were only checking correlations of numeric values: it shouldnt matter at this point but conver male/female to 0-1, frequency, dates etc?"""

#check correlations for transactions
rs = np.random.RandomState(0)
df = pd.DataFrame(rs.rand(10, 10))
corr = transactions.corr()
corr.style.background_gradient(cmap='coolwarm')

"""No major correlations: mostly between transaction ID's and account ID's as many transactions are unique for an account."""

#check correlations for clients
rs = np.random.RandomState(0)
df = pd.DataFrame(rs.rand(10, 10))
corr = clients.corr()
corr.style.background_gradient(cmap='coolwarm')

"""Very high correlations with associated ID's (we've already done merge on the associated dispositions). Significant correlation between client district id and and branch district id as in most cases they are the same."""

#check correlations for districts
rs = np.random.RandomState(0)
df = pd.DataFrame(rs.rand(10, 10))
corr = districts.corr()
corr.style.background_gradient(cmap='coolwarm')

"""Conclusions about correlation for districts:

* Data about sucessive years have very high correlation -> Save only one year and extract a value for growth

* Absolute values (no of crimes) has very high correlation with population -> Turn into crimerate with division

Since some values related to the number of crimes committed in 95 and 96 were missing, we also use the K-Nearest Neighbor algorithm to fill in empty values in the newly created crime growth column and unemployment growth column.
"""

#calc crimerates rather than whole values
districts['crimerate95'] = districts.apply(lambda row: row["no. of commited crimes '95 "] / row["no. of inhabitants"], axis=1)
districts['crimerate96'] = districts.apply(lambda row: row["no. of commited crimes '96 "] / row["no. of inhabitants"], axis=1)
#calculate with growth rate from 95-96
districts['crimegrowth'] = districts.apply(lambda row: row["crimerate96"] / row["crimerate95"], axis=1)
districts['unempgrowth'] = districts.apply(lambda row: row["unemploymant rate '96 "] / row["unemploymant rate '95 "], axis=1)
 
#drops columns now calculated with relative vals and growth
districts.drop(columns="crimerate95", inplace=True)
districts.drop(columns="no. of commited crimes '95 ", inplace=True)
districts.drop(columns="no. of commited crimes '96 ", inplace=True)
districts.drop(columns="unemploymant rate '95 ", inplace=True)
 
#clean up names for better experience
districts = districts.rename(columns={"no. of inhabitants": "population",
                   "no. of municipalities with inhabitants < 499 ": "munip<499",
                   "no. of municipalities with inhabitants 500-1999": "munip500-1999",
                   "no. of municipalities with inhabitants 2000-9999 ": "munip2000-9999",
                   "no. of municipalities with inhabitants >10000 ": "munip>10000",
                   "no. of municipalities with inhabitants >10000 ": "munip>10000",
                   "no. of cities ": "cities",
                   "ratio of urban inhabitants ": "urbanization",
                   "average salary ": "avg_salary",
                   "unemploymant rate '96 ": "unemployment96",
                   "no. of enterpreneurs per 1000 inhabitants ": "entrepreneurs/1000"
                  })
 
#handle missing vals remaining in crimegrowth, unempgrowth with K-nearest neighbour
from sklearn.impute import KNNImputer
from sklearn.preprocessing import MinMaxScaler
df_knn = districts.copy()
df_knn.drop(columns=["code", "name", "region"], inplace=True)
#scale values from 0-1 for KNN
scaler = MinMaxScaler(feature_range=(0, 1))
df_knn = pd.DataFrame(scaler.fit_transform(df_knn), columns = df_knn.columns)
#target values scaled, reset
df_knn['crimegrowth'] = districts['crimegrowth']
df_knn['unempgrowth'] = districts['unempgrowth']
#impute knn
knn_imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')
df_knn_imputed = pd.DataFrame(knn_imputer.fit_transform(df_knn), columns=df_knn.columns)
 
districts.fillna(df_knn_imputed, inplace=True)

"""Moving on to transactions, we combine transactions of each client by determining their accounts balance at 4 moments in time (the most recent balance, the balance 1, 6 and 12 months before that most recent date)"""

transaction_index = {} #['account_id] = {RecentBalance, MonthBalance, 3MonthBalance 6MonthBalance, 12MonthBalance, ...type of transaction...}

transactions_bydate = transactions.sort_values(by="date", ascending=False)



for index, transaction in transactions_bydate.iterrows():
    account_id = transaction["account_id"]
    if account_id not in transaction_index.keys():
        transaction_index[account_id] = {}
        transaction_index[account_id]["num_salaries"] = 0
        transaction_index[account_id]["average_salary"] = 0
        transaction_index[account_id]["last_amount"] =  transaction["balance"]
        transaction_index[account_id]["recent_balance"] = transaction
        transaction_index[account_id]["household_payments"] = 0
        transaction_index[account_id]["payment_for_statement_counter"] = 0
        transaction_index[account_id]["insurrance_payments"] = 0       
        transaction_index[account_id]["sanction_payment_counter"] = 0
        transaction_index[account_id]["old_age_pensions"] = 0


    else:

    
        delta = transaction_index[account_id]["recent_balance"]["date"] - transaction["date"]
        #print(type(delta))

        if delta.days > 30*(transaction_index[account_id]["num_salaries"]+1):
          prev_avg_salary = transaction_index[account_id]["average_salary"] 
          num_salaries = transaction_index[account_id]["num_salaries"]
          new_salary = transaction_index[account_id]["last_amount"] - transaction["balance"]

          transaction_index[account_id]["average_salary"] = (prev_avg_salary*num_salaries + new_salary)/(num_salaries+1)
          transaction_index[account_id]["last_amount"] = transaction["balance"]
          transaction_index[account_id]["num_salaries"] += 1

          #Alternative way to get monthly balance of all months
          #month_balance_str = str(transaction_index[account_id]["num_salaries"]) + "_month_balance"
          #transaction_index[account_id][month_balance_str] = transaction["balance"]

        #Alternative way of getting the monthly balance of specific months
        if "month_balance" not in transaction_index[account_id].keys():
            if delta.days > 30:
                transaction_index[account_id]["month_balance"] = transaction["balance"]
        elif "3_month_balance" not in transaction_index[account_id].keys():
            if delta.days > 90: 
              transaction_index[account_id]["3_month_balance"] = transaction["balance"]
        elif "6_month_balance" not in transaction_index[account_id].keys():
            if delta.days > 180:
                transaction_index[account_id]["6_month_balance"] = transaction["balance"]
        elif "12_month_balance" not in transaction_index[account_id].keys():
            if delta.days > 365: 
                transaction_index[account_id]["12_month_balance"] = transaction["balance"]

    if transaction["k_symbol"] == "household":
      transaction_index[account_id]["household_payments"] += 1
    elif transaction["k_symbol"] == "payment for statement":
      transaction_index[account_id]["payment_for_statement_counter"] += 1
    elif transaction["k_symbol"] == "insurrance payment":
      transaction_index[account_id]["insurrance_payments"] += 1  
    elif transaction["k_symbol"] == "sanction interest if negative balance":
      transaction_index[account_id]["sanction_payment_counter"] += 1
    elif transaction["k_symbol"] == "old-age pension":
      transaction_index[account_id]["old_age_pensions"] += 1


for index in transaction_index.keys():
    transaction_index[index]["recent_balance"] = transaction_index[index]["recent_balance"]["balance"]

"""We then merge the account balances into the Clients dataset"""

#merge the balance dataset (transaction_index) with the accounts dataset to get the final accounts dataset with balance information
for account_id in transaction_index.keys():
    transaction_index[account_id]["account_id"] = int(account_id)

transaction_index_df = pd.DataFrame(transaction_index)

transaction_index_df = transaction_index_df.transpose()
transaction_index_df.pop("last_amount")



new_clients = pd.merge(clients, transaction_index_df, on="account_id", how="outer")
new_clients
new_clients.to_csv('./new_clients.csv', index = False, header=True)

"""Next, we merge the Loans and Clients datasets and add 3 new columns: the age of the client at the time of the loan, the age of the account at the time of the loan and the age of the card at the time of the loan. These columns are derived from the subtraction of the birth date, creation date and card issue date columns from the date of the loan column, respectively, and allow us to determine correlation between these values and the other columns of the merged dataset."""

#assuming all data treated at this point
loans = pd.merge(loans, new_clients, on="account_id", how="outer")
loans = pd.merge(loans, districts, left_on='client_district_id', right_on='code')
loans = pd.merge(loans, districts, left_on='branch_district_id', right_on='code', suffixes=('', '_branch'))
loans['age_when_loan'] = loans.apply(lambda row: (row['date'] - row['birthdate']), axis=1)
loans['age_when_loan'] = loans['age_when_loan'].dt.days
loans['account_age_when_loan'] = loans.apply(lambda row: (row['date'] - row['creation']), axis=1)
loans['account_age_when_loan'] = loans['account_age_when_loan'].dt.days
loans['card_age_when_loan'] = loans.apply(lambda row: (row['date'] - row['card_issue']), axis=1)
loans['card_age_when_loan'] = loans['card_age_when_loan'].dt.days

#drop ids used for merge
loans.drop(columns=["account_id", "disp_id", "client_id", "branch_district_id", "client_district_id"], inplace=True)

loans.to_csv('./final_data.csv', index = False, header=True)

"""Now that we have a dataset that combines all the initial datasets, we define a function for removing outliers, based on 1.5x inter-quartile rule. After that we apply it to the amount and duration columns - isto tem de ser melhorado (?)"""

def remove_outliers(df,column):
    Q1 = np.percentile(df[column], 25,
                    method = 'midpoint')
    
    Q3 = np.percentile(df[column], 75,
                    method = 'midpoint')
    IQR = Q3 - Q1
    # Upper bound
    upper = np.where(df[column] >= (Q3+1.5*IQR))
    # Lower bound
    lower = np.where(df[column] <= (Q1-1.5*IQR))
    df.drop(upper[0], inplace = True)
    df.drop(lower[0], inplace = True)


remove_outliers(loans,'amount')
remove_outliers(loans,'duration')

#check correlations for loans
rs = np.random.RandomState(0)
df = pd.DataFrame(rs.rand(10, 10))
corr = loans.corr()
corr.style.background_gradient(cmap='coolwarm')